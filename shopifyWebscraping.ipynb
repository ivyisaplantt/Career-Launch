{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdJdPboUPJLmInh3fUXBuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyisaplantt/Career-Launch/blob/main/shopifyWebscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code from https://scrapfly.io/blog/crawling-with-python/"
      ],
      "metadata": {
        "id": "ILD6cKSZNm2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx parsel w3lib tldextract loguru"
      ],
      "metadata": {
        "id": "cPDKuH-lTYYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bd5973-7f44-4e29-e178-c01149454db6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting parsel\n",
            "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting w3lib\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Collecting cssselect>=1.2.0 (from parsel)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jmespath (from parsel)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from parsel) (5.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from parsel) (24.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: w3lib, loguru, jmespath, cssselect, requests-file, parsel, tldextract\n",
            "Successfully installed cssselect-1.2.0 jmespath-1.0.1 loguru-0.7.3 parsel-1.9.1 requests-file-2.1.0 tldextract-5.1.3 w3lib-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for extractor function\n",
        "from typing import List\n",
        "from urllib.parse import urljoin\n",
        "from parsel import Selector\n",
        "import httpx"
      ],
      "metadata": {
        "id": "rAyJRf5EPz8b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for filter class\n",
        "from typing import Pattern\n",
        "import posixpath\n",
        "from urllib.parse import urlparse\n",
        "from tldextract import tldextract\n",
        "from w3lib.url import canonicalize_url\n",
        "from loguru import logger as log"
      ],
      "metadata": {
        "id": "AXnx9DNkVYSv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for Crawler class\n",
        "import asyncio\n",
        "from typing import Callable, Dict, Optional, Tuple\n",
        "# from Filter import UrlFilter"
      ],
      "metadata": {
        "id": "NfuYXGXkc5Vl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for nytimes implementation\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "yr26fkYcg6PR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url extractor function\n",
        "def extract_urls(response: httpx.Response) -> List[str]:\n",
        "  tree = Selector(text=response.text)\n",
        "  # using XPath\n",
        "  urls = tree.xpath('//a/@href').getall()\n",
        "  # or CSS\n",
        "  urls = tree.css('a::attr(href)').getall()\n",
        "  # turn relative urls (/foo.html) to absolute (https://domain.com/foo.html)\n",
        "  urls = [urljoin(str(response.url), url.strip()) for url in urls]\n",
        "  return urls"
      ],
      "metadata": {
        "id": "ScPL_YJsUJ6A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the url extractor function\n",
        "response = httpx.get(\"http://httpbin.org/links/10/1\")\n",
        "for url in extract_urls(response):\n",
        "  print(url)"
      ],
      "metadata": {
        "id": "xPqMmhezUkdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8382a973-6d84-4fd7-f9c2-96e36498f956"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://httpbin.org/links/10/0\n",
            "http://httpbin.org/links/10/2\n",
            "http://httpbin.org/links/10/3\n",
            "http://httpbin.org/links/10/4\n",
            "http://httpbin.org/links/10/5\n",
            "http://httpbin.org/links/10/6\n",
            "http://httpbin.org/links/10/7\n",
            "http://httpbin.org/links/10/8\n",
            "http://httpbin.org/links/10/9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url filter class\n",
        "class UrlFilter:\n",
        "  IGNORED_EXTENSIONS = [\n",
        "        # archives\n",
        "        '7z', '7zip', 'bz2', 'rar', 'tar', 'tar.gz', 'xz', 'zip',\n",
        "        # images\n",
        "        'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif', 'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg', 'cdr', 'ico',\n",
        "        # audio\n",
        "        'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n",
        "        # video\n",
        "        '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv', 'm4a', 'm4v', 'flv', 'webm',\n",
        "        # office suites\n",
        "        'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',\n",
        "        # other\n",
        "        'css', 'pdf', 'exe', 'bin', 'rss', 'dmg', 'iso', 'apk',\n",
        "    ]\n",
        "\n",
        "  def __init__(self, domain:str=None, subdomain:str=None, follow:List[Pattern]=None) -> None:\n",
        "    # restrict filtering to specific TLD\n",
        "    self.domain = domain or \"\"\n",
        "    # restrict filtering to specific subdomain\n",
        "    self.subdomain = subdomain or \"\"\n",
        "    self.follow = follow or []\n",
        "    log.info(f\"filter created for domain {self.subdomain}.{self.domain} with follow rules {follow}\")\n",
        "    self.seen = set()\n",
        "\n",
        "  def is_valid_ext(self, url):\n",
        "    # ignore non-crawlable documents\n",
        "    return posixpath.splitext(urlparse(url).path)[1].lower() not in self.IGNORED_EXTENSIONS\n",
        "\n",
        "  def is_valid_scheme(self, url):\n",
        "    # ignore non http/s links\n",
        "    return urlparse(url).scheme in [\"http\", \"https\"]\n",
        "\n",
        "  def is_valid_domain(self, url):\n",
        "    # ignore offsite urls (only keep urls with same domain and subdomain)\n",
        "    parsed = tldextract.extract(url)\n",
        "    return parsed.registered_domain == self.domain and parsed.subdomain == self.subdomain\n",
        "\n",
        "  def is_valid_path(self, url):\n",
        "    # ignore urls of undesired paths\n",
        "    if not self.follow:\n",
        "      return True\n",
        "    path = urlparse(url).path\n",
        "    for pattern in self.follow:\n",
        "      if pattern.match(path):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def is_new(self, url):\n",
        "    # ignore visited urls\n",
        "    return canonicalize_url(url) not in self.seen\n",
        "\n",
        "  def filter(self, urls: List[str]) -> List[str]:\n",
        "    # filter list of urls\n",
        "    found = []\n",
        "    for url in urls:\n",
        "      if not self.is_valid_scheme(url):\n",
        "        log.debug(f\"drop ignored scheme {url}\")\n",
        "        continue\n",
        "      if not self.is_valid_domain(url):\n",
        "        log.debug(f\"drop domain mismatch {url}\")\n",
        "        continue\n",
        "      if not self.is_valid_ext(url):\n",
        "        log.debug(f\"drop ignored extension {url}\")\n",
        "        continue\n",
        "      if not self.is_valid_path(url):\n",
        "        log.debug(f\"drop ignored path {url}\")\n",
        "        continue\n",
        "      if not self.is_new(url):\n",
        "        log.debug(f\"drop duplicate {url}\")\n",
        "        continue\n",
        "      self.seen.add(canonicalize_url(url))\n",
        "      found.append(url)\n",
        "    return found"
      ],
      "metadata": {
        "id": "l9HpwILRVmWR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the url filter function\n",
        "nytimes_filter = UrlFilter(\"nytimes.com\", \"store\")\n",
        "response = httpx.get(\"https://store.nytimes.com\")\n",
        "urls = extract_urls(response)\n",
        "filtered = nytimes_filter.filter(urls)\n",
        "filtered_2nd_page = nytimes_filter.filter(urls)\n",
        "print(filtered)\n",
        "print(filtered_2nd_page)"
      ],
      "metadata": {
        "id": "8rCd4RtiX8K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawler class\n",
        "class Crawler:\n",
        "  async def __aenter__(self):\n",
        "    self.session = await httpx.AsyncClient(\n",
        "        timeout=httpx.Timeout(60.0),\n",
        "        limits=httpx.Limits(max_connections=5),\n",
        "        headers={\n",
        "            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
        "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"accept-language\": \"en-US;en;q=0.9\",\n",
        "            \"accept-encoding\": \"gzip, deflate, br\",\n",
        "        },\n",
        "    ).__aenter__()\n",
        "    return self\n",
        "\n",
        "  async def __aexit__(self, *args, **kwargs):\n",
        "    await self.session.__aexit__(*args, **kwargs)\n",
        "\n",
        "  def __init__(self, filter: UrlFilter, callbacks: Optional[Dict[str, Callable]] = None) -> None:\n",
        "    self.url_filter = filter\n",
        "    self.callbacks = callbacks or {}\n",
        "\n",
        "  def parse(self, response: List[httpx.Response]) -> List[str]:\n",
        "    # find valid urls in responses\n",
        "    all_unique_urls = set()\n",
        "    found = []\n",
        "    for response in responses:\n",
        "      sel = Selector(text=response.text, base_url=str(response.url))\n",
        "      _url_in_response = set(\n",
        "          urljoin(str(response.url), url.strip())\n",
        "          for url in sel.xpath(\"//a/@href\").getall()\n",
        "      )\n",
        "      all_unique_urls |= _url_in_response\n",
        "    urls_to_follow = self.url_filter.filter(all_unique_urls)\n",
        "    log.info(f\"found {len(urls_to_follow)} urls to follow (from total {len(all_unique_urls)})\")\n",
        "    return urls_to_follow\n",
        "\n",
        "  async def scrape_url(self, url):\n",
        "    return await self.session.get(url, follow_redirects=True)\n",
        "\n",
        "  async def scrape(self, urls: List[str]) -> Tuple[List[httpx.Response], List[Exception]]:\n",
        "    # scrape urls and return their responses\n",
        "    responses = []\n",
        "    failures = []\n",
        "    log.info(f\"scraping {len(urls)} urls\")\n",
        "\n",
        "    tasks = [self.scrape_url(url) for url in urls]\n",
        "    for result in await asyncio.gather(*tasks, return_exceptions=True):\n",
        "      if isinstance(result, httpx.Response):\n",
        "        responses.append(result)\n",
        "      else:\n",
        "        failures.append(result)\n",
        "    return responses, failures\n",
        "\n",
        "  async def run(self, starts_urls: List[str], max_depth=5) -> None:\n",
        "    # crawl target to maximum depth or until no more urls are found\n",
        "    url_pool = starts_urls\n",
        "    depth = 0\n",
        "    while url_pool and depth <= max_depth:\n",
        "      responses, failures= await self.scrape(url_pool)\n",
        "      log.info(f\"depth {depth}: scraped {len(responses)} pages and failed {len(failures)}\")\n",
        "      url_pool = self.parse(responses)\n",
        "      await self.callback(responses)\n",
        "      depth += 1\n",
        "\n",
        "  async def callback(self, responses):\n",
        "    for response in responses:\n",
        "      for pattern, fn in self.callbacks.items():\n",
        "        if pattern.match(str(response.url)):\n",
        "          log.debug(f'found matching callback for {response.url}')\n",
        "          fn(response=response)\n"
      ],
      "metadata": {
        "id": "jczIxTW9dL2N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_json_objects(text: str, decoder=json.JSONDecoder()):\n",
        "    # Find JSON objects in text, and yield the decoded JSON data\n",
        "    pos = 0\n",
        "    while True:\n",
        "        match = text.find('{', pos)\n",
        "        if match == -1:\n",
        "            break\n",
        "        try:\n",
        "            result, index = decoder.raw_decode(text[match:])\n",
        "            yield result\n",
        "            pos = match + index\n",
        "        except ValueError:\n",
        "            pos = match + 1\n",
        "\n",
        "def find_json_in_script(response: httpx.Response, keys):\n",
        "    # find all json objects in HTML <script> tags that contain specified keys\n",
        "    scripts = Selector(text=response.text).xpath('//script/text()').getall()\n",
        "    objects = []\n",
        "    for script in scripts:\n",
        "        if not all(f'\"{k}\"' in script for k in keys):\n",
        "            continue\n",
        "        objects.extend(extract_json_objects(script))\n",
        "    return [obj for obj in objects if all(k in str(obj) for k in keys)]"
      ],
      "metadata": {
        "id": "qJkilkvdru3w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing above code\n",
        "url = \"https://store.nytimes.com/collections/apparel/products/a1-stacked-logo-shirt\"\n",
        "response = httpx.get(url)\n",
        "products = find_json_in_script(response, [\"published_at\", \"price\"])\n",
        "print(json.dumps(products, indent=2, ensure_ascii=False)[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-VNZydrr7Id",
        "outputId": "5f92b863-ec27-4aee-89b7-d33a38f45993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": 6984160215110,\n",
            "    \"title\": \"Stacked Logo Shirt\",\n",
            "    \"handle\": \"a1-stacked-logo-shirt\",\n",
            "    \"description\": \"<p>Wear The Times proudly with this simple yet expressive T-shirt, featuring the world-renowned New York Times logo, stacked on three lines. This 100% cotton short-sleeve shirt has a comfortable unisex fit and is available in black and gray, in sizes ranging from XS to 4XL. It also comes in kids sizes for budding young journalists.</p>\\n<!-- split -->\\n<p>The Times logo ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nlz_NcmPNb0x"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "def parse_product(response):\n",
        "    products = find_json_in_script(response, [\"published_at\", \"price\"])\n",
        "    results.extend(products)\n",
        "    if not products:\n",
        "        log.warning(f\"could not find product data in {response.url}\")\n",
        "\n",
        "\n",
        "\n",
        "async def run():\n",
        "    callbacks = {\n",
        "        # any url that contains \"/products/\" is a product page\n",
        "        re.compile(\".+/products/.+\"): parse_product\n",
        "    }\n",
        "    url_filter = UrlFilter(domain=\"nytimes.com\", subdomain=\"store\")\n",
        "    async with Crawler(url_filter, callbacks=callbacks) as crawler:\n",
        "        await crawler.run([\"https://store.nytimes.com/\"])\n",
        "    print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is getting an error\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(run())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "1kshXZSBd6RH",
        "outputId": "3440a2d2-3ddf-4919-c239-267a608a6956"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run() missing 1 required positional argument: 'main'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-45bddb622034>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: run() missing 1 required positional argument: 'main'"
          ]
        }
      ]
    }
  ]
}